{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# LSTM for POS tagging Tutorial\n",
    "\n",
    "Unlike traditional machine learning methods, where features are dependent on specialize knowledge. Deep learning can automaticallly extract features. In NLP, one of the most common deep learning architecture is LSTM. It is designed to capture long-term dependencies between words. In this tutorial, we will show you how to code LSTM model for a POS tagger system from (almost) scratch using PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1.11_5\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from itertools import chain\n",
    "import nltk\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "import sklearn\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "print(torch.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Brown Corpus\n",
    "The Brown Corpus of Standard American English was the first million-word computer readable corpus created in 1961 at Brown University. We use the Universal Part-of-Speech tagset in this tutorial. Knowing which Part-Of-Speech a word belongs to tells us  about its grammatical function in the sentence and its likely neighboring words. Therefore,  POS tagging is an important process in many NLP applications (e.g. Word Sense Disambiguation, Named entity Recognition).\n",
    "\n",
    "\n",
    "\n",
    "Universal Part-of-Speech Tagset \n",
    "\n",
    "|Tag | \tMeaning |\tEnglish Examples|\n",
    "| --- | --- | --- |\n",
    "|ADJ \t|adjective \t|new, good, high, special, big, local|\n",
    "|ADP \t|adposition \t|on, of, at, with, by, into, under|\n",
    "|ADV \t|adverb \t|really, already, still, early, now|\n",
    "|CONJ \t|conjunction |and, or, but, if, while, although|\n",
    "|DET \t|determiner, article |the, a, some, most, every, no, which|\n",
    "|NOUN \t|noun \t|year, home, costs, time, Africa|\n",
    "|NUM \t|numeral \t|twenty-four, fourth, 1991, 14:24|\n",
    "|PRT \t|particle \t|at, on, out, over per, that, up, with|\n",
    "|PRON \t|pronoun \t|he, their, her, its, my, I, us|\n",
    "|VERB \t|verb \t|is, say, told, given, playing, would|\n",
    "|. \t|punctuation marks \t|. , ; !|\n",
    "|X \t|other \t|ersatz, esprit, dunno, gr8, univeristy|\n",
    "\n",
    "\n",
    "(Reference: http://www.nltk.org/book/ch05.html)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tagged_sents=nltk.corpus.brown.tagged_sents(tagset='universal') #load the corpus from NLTK library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#split data into train set and test set\n",
    "train_size = int(len(tagged_sents) * 0.9)\n",
    "train_sents = tagged_sents[:train_size]\n",
    "test_sents = tagged_sents[train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 'DET'),\n",
       " ('Fulton', 'NOUN'),\n",
       " ('County', 'NOUN'),\n",
       " ('Grand', 'ADJ'),\n",
       " ('Jury', 'NOUN'),\n",
       " ('said', 'VERB'),\n",
       " ('Friday', 'NOUN'),\n",
       " ('an', 'DET'),\n",
       " ('investigation', 'NOUN'),\n",
       " ('of', 'ADP'),\n",
       " (\"Atlanta's\", 'NOUN'),\n",
       " ('recent', 'ADJ'),\n",
       " ('primary', 'NOUN'),\n",
       " ('election', 'NOUN'),\n",
       " ('produced', 'VERB'),\n",
       " ('``', '.'),\n",
       " ('no', 'DET'),\n",
       " ('evidence', 'NOUN'),\n",
       " (\"''\", '.'),\n",
       " ('that', 'ADP'),\n",
       " ('any', 'DET'),\n",
       " ('irregularities', 'NOUN'),\n",
       " ('took', 'VERB'),\n",
       " ('place', 'NOUN'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#a sample sentence from the training set\n",
    "train_sents[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the Data: Words, POS tags\n",
    "\n",
    "\n",
    "indexing!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_list=[]\n",
    "pos_list=[]\n",
    "\n",
    "for sent in train_sents:\n",
    "    for word in sent:\n",
    "        word_list.append(word[0])\n",
    "        pos_list.append(word[1])\n",
    "\n",
    "\n",
    "#Word to Index\n",
    "word_list.append(\"UNK\") #Special Token for unknown words\n",
    "all_words = sorted(set(word_list))\n",
    "all_pos = sorted(set(pos_list))\n",
    "del word_list, pos_list\n",
    "word_to_ix = dict((c, i) for i, c in enumerate(all_words)) #convert word to index \n",
    "pos_to_ix = dict((c, i) for i, c in enumerate(all_pos)) #convert pos to index\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'.': 0, 'ADJ': 1, 'ADP': 2, 'ADV': 3, 'CONJ': 4, 'DET': 5, 'NOUN': 6, 'NUM': 7, 'PRON': 8, 'PRT': 9, 'VERB': 10, 'X': 11}\n"
     ]
    }
   ],
   "source": [
    "print(pos_to_ix) # POS tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ix_to_word = dict((v,k) for k,v in word_to_ix.items()) #convert index to word\n",
    "ix_to_pos = dict((v,k) for k,v in pos_to_ix.items())  #convert index to word\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the Data for PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', 'Friday', 'an', 'investigation', 'of', \"Atlanta's\", 'recent', 'primary', 'election', 'produced', '``', 'no', 'evidence', \"''\", 'that', 'any', 'irregularities', 'took', 'place', '.'] \n",
      " ['DET', 'NOUN', 'NOUN', 'ADJ', 'NOUN', 'VERB', 'NOUN', 'DET', 'NOUN', 'ADP', 'NOUN', 'ADJ', 'NOUN', 'NOUN', 'VERB', '.', 'DET', 'NOUN', '.', 'ADP', 'DET', 'NOUN', 'VERB', 'NOUN', '.']\n"
     ]
    }
   ],
   "source": [
    "#split each input from its target\n",
    "\n",
    "input_sent =[ [ word[0] for word in sent]for sent in train_sents ] #words only\n",
    "train_targets =[ [ word[1] for word in sent]for sent in train_sents ] #POS only\n",
    "\n",
    "input_test_sent =[ [ word[0] for word in sent]for sent in test_sents ] #words only\n",
    "test_targets =[ [ word[1] for word in sent]for sent in test_sents ] #POS only\n",
    "print(input_sent[0], \"\\n\", train_targets[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Functions for converting input/target text sequences to PyTorch-compatible sequences \n",
    "\n",
    "def prepare_sequence_word(input_text):\n",
    "    \"\"\"\n",
    "    Convert an input text sequence for training phrase\n",
    "    \"\"\"\n",
    "    idxs = [word_to_ix[w] for w in input_text]\n",
    "    tensor = torch.LongTensor(idxs)\n",
    "    return Variable(tensor).cuda()\n",
    "\n",
    "def prepare_sequence_word_test(input_text):\n",
    "    \"\"\"\n",
    "    Convert an input text sequence for testing phrase\n",
    "    \"\"\"\n",
    "    idxs = []\n",
    "    for word in input_text:\n",
    "        if word in word_to_ix:\n",
    "            idxs.append(word_to_ix[word])\n",
    "        else:\n",
    "            idxs.append(word_to_ix[\"UNK\"]) #Use UNK tag for unknown word\n",
    "   \n",
    "    tensor = torch.LongTensor(idxs)\n",
    "    return Variable(tensor, volatile=True).cuda()\n",
    "\n",
    "def prepare_sequence_target_pos(input_label):\n",
    "    \"\"\"\n",
    "    Convert an input target sequence for training phrase\n",
    "    \"\"\"\n",
    "    idxs = [pos_to_ix[w] for w in input_label]\n",
    "    tensor = torch.LongTensor(idxs)\n",
    "    return Variable(tensor).cuda()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The LSTM Neural Network Architecture for POS tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, word_emb_dim , hidden_dim, output_size):\n",
    "        super(LSTM, self).__init__()\n",
    "\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.word_embedding = nn.Embedding(input_size, word_emb_dim)\n",
    "\n",
    "        self.i2ig = nn.Linear(word_emb_dim, hidden_dim) #inputgate\n",
    "        self.h2ig = nn.Linear(hidden_dim, hidden_dim) #inputgate\n",
    "        \n",
    "        self.i2ctil = nn.Linear(word_emb_dim, hidden_dim) #c_tilda (new memory cell)\n",
    "        self.h2ctil = nn.Linear(hidden_dim, hidden_dim) #c_tilda (new memory cell)\n",
    "        \n",
    "        self.i2fg = nn.Linear(word_emb_dim, hidden_dim) #forgetgate\n",
    "        self.h2fg = nn.Linear(hidden_dim, hidden_dim) #forgetgate\n",
    "        \n",
    "        self.i2og = nn.Linear(word_emb_dim, hidden_dim) #outputgate\n",
    "        self.h2og = nn.Linear(hidden_dim, hidden_dim) #outputgate\n",
    "        \n",
    "        self.hidden2tag = nn.Linear(hidden_dim, output_size) #output\n",
    "        self.softmax = nn.LogSoftmax()\n",
    "        \n",
    "        \n",
    "    def _inputGate(self, word_embed, hidden):\n",
    "        \"\"\"\n",
    "        input gate: how much the current input matters according to the past hidden state \n",
    "        and the current input\n",
    "        \"\"\"\n",
    "        i_t = F.sigmoid(self.i2ig(word_embed)+self.h2ig(hidden))\n",
    "        return i_t\n",
    "                        \n",
    "    def _newMemoryCell(self, word_embed, hidden):\n",
    "        \"\"\"\n",
    "        New memory cell: how much the current input will be remembered according to the past hidden state \n",
    "        and the current input \n",
    "        \"\"\"\n",
    "        c_tilda = F.tanh(self.i2ctil(word_embed)+self.h2ctil(hidden)) #new memory cell\n",
    "        return c_tilda\n",
    "    \n",
    "    def _forgetGate(self, word_embed, hidden):\n",
    "        \"\"\"\n",
    "        Forget gate: how much the past memory cell matters according to the past hidden state \n",
    "        and the current input\n",
    "        \"\"\"\n",
    "        f_t = F.sigmoid(self.i2fg(word_embed)+self.h2fg(hidden))#forget gate\n",
    "        return f_t\n",
    "    \n",
    "    def _cellState(self,c_prev,c_tilda,f_t,i_t):\n",
    "        \"\"\"\n",
    "        Final memory cell: Decide how much the past memory cell will be forgotten according to the forget gate\n",
    "        and decide how much the new memory cell matters according to the input gate. \n",
    "        Then it sums both decisions to generate the final memory cell (c_t)\n",
    "        \"\"\"\n",
    "        c_t = (f_t * c_prev) + (i_t * c_tilda) \n",
    "        return c_t\n",
    "    \n",
    "    def _outputGate(self, word_embed, hidden, c_t):\n",
    "        \n",
    "        \"\"\"\n",
    "        Output gate: calculate the final hidden state according to the current input, the past hidden state\n",
    "        , and the final memory cell\n",
    "        \"\"\"\n",
    "        o_t =   F.sigmoid(self.i2og(word_embed)+self.h2og(hidden))\n",
    "        h_t  =  o_t * F.tanh(c_t)\n",
    "        return h_t \n",
    "\n",
    "    def forward(self, input, hidden, c_prev):\n",
    "   \n",
    "        word_embed = self.word_embedding(input)\n",
    "        f_t = self._forgetGate(word_embed, hidden)\n",
    "        i_t = self._inputGate(word_embed, hidden)\n",
    "        c_tilda = self._newMemoryCell(word_embed, hidden)\n",
    "        c_t = self._cellState(c_prev, c_tilda, f_t, i_t)#final memory cell\n",
    "        h_t = self._outputGate(word_embed, hidden,c_t)#final hidden state\n",
    "        lstm_out = self.hidden2tag(h_t)\n",
    "        output = self.softmax(lstm_out)\n",
    "        return output, h_t, c_t\n",
    "\n",
    "    def initHidden(self):\n",
    "        return Variable(torch.zeros(1, self.hidden_dim)).cuda()\n",
    "    \n",
    "    def initCellState(self):\n",
    "        return Variable(torch.zeros(1, self.hidden_dim)).cuda()\n",
    "\n",
    "#Initialization    \n",
    "n_words = len(word_to_ix) #number of unique words\n",
    "n_hidden = 128 # number of hidden dimensions\n",
    "n_emb_dim = 64 # number of word vector dimension\n",
    "n_categories = len(pos_to_ix) # number of POS categories\n",
    "lstm = LSTM(n_words, n_emb_dim, n_hidden, n_categories).cuda() #initilize the model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "criterion = nn.NLLLoss() #Last layer is nn.LogSoftMax, therefore NLLLoss is suitable\n",
    "learning_rate = 0.001\n",
    "optimizer = torch.optim.Adam(lstm.parameters(), lr=learning_rate) #ADAM OPTIMIZER\n",
    "\n",
    "def train(category_tensor, sentence_tensor):\n",
    "    lstm.train(True)\n",
    "    hidden = lstm.initHidden() #initialize hidden state\n",
    "    cell_state = lstm.initCellState() #initiailze cell state\n",
    "    lstm.zero_grad()# reset gradient to zero\n",
    "\n",
    "    for i in range(sentence_tensor.size()[0]): # for each word in a sentence\n",
    "        output, hidden, cell_state = lstm(sentence_tensor[i], hidden,cell_state)#fwd\n",
    "        if i != 0:\n",
    "            all_outputs=torch.cat((all_outputs,output),0) #concat output vectors together to calculate loss in one go\n",
    "        else:\n",
    "            all_outputs=output\n",
    "\n",
    "    loss = criterion(all_outputs, category_tensor)# calculate loss\n",
    "    loss.backward()#backprop\n",
    "    optimizer.step()#update parameters\n",
    "    return all_outputs, loss.data[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 5% (25m 53s) 0.3268 \n",
      "2 10% (51m 3s) 0.1043 \n",
      "3 15% (76m 17s) 0.1014 \n",
      "4 20% (101m 4s) 0.0904 \n",
      "5 25% (125m 52s) 0.1094 \n",
      "6 30% (150m 39s) 0.0720 \n",
      "7 35% (175m 28s) 0.0555 \n",
      "8 40% (200m 16s) 0.0139 \n",
      "9 45% (225m 4s) 0.0235 \n",
      "10 50% (249m 53s) 0.0172 \n",
      "11 55% (274m 41s) 0.0227 \n",
      "12 60% (299m 29s) 0.0093 \n",
      "13 65% (324m 16s) 0.0100 \n",
      "14 70% (349m 4s) 0.0226 \n",
      "15 75% (373m 52s) 0.0685 \n",
      "16 80% (398m 40s) 0.1013 \n",
      "17 85% (423m 27s) 0.0151 \n",
      "18 90% (448m 15s) 0.0043 \n",
      "19 95% (473m 3s) 0.0077 \n",
      "20 100% (498m 39s) 0.0064 \n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "n_iters = 20\n",
    "print_every = 1\n",
    "\n",
    "def timeSince(since):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for iter in range(1, n_iters + 1):#Epoch\n",
    "    for sentence, pos_tags in zip(input_sent,train_targets): #Sentence\n",
    "        sentence_tensor = prepare_sequence_word(sentence) #preprocess input\n",
    "        category_tensor = prepare_sequence_target_pos(pos_tags) #preprocess tags\n",
    "        output, loss = train(category_tensor, sentence_tensor) #fwd\n",
    "\n",
    "\n",
    "    # Print iter number,time, loss\n",
    "    if iter % print_every == 0:\n",
    "        \n",
    "        print('%d %d%% (%s) %.4f ' % (iter, iter / n_iters * 100, timeSince(start), loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save/Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save model\n",
    "#torch.save(lstm.state_dict(), \"mylstm.pt\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load model\n",
    "lstm = LSTM(n_words, n_emb_dim, n_hidden, n_categories).cuda() #initilize the model\n",
    "lstm.load_state_dict(torch.load(\"mylstm.pt\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(input_sent):\n",
    "    y_pred=[]\n",
    "    lstm.train(False)\n",
    "    hidden = lstm.initHidden()\n",
    "    cell_state = lstm.initCellState()\n",
    "    sentence_tensor = prepare_sequence_word_test(input_sent)\n",
    "    for i in range(sentence_tensor.size()[0]):\n",
    "        output, hidden, cell_state = lstm(sentence_tensor[i], hidden,cell_state)\n",
    "        output=output[0].data.tolist()\n",
    "        out_ix=output.index(max(output))\n",
    "        y_pred.append(ix_to_pos[out_ix])\n",
    "    \n",
    "    return y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.', 'DET', 'ADJ', 'NOUN', '.', 'DET', 'NOUN', '.', '.']"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(input_test_sent[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('``', '.'),\n",
       " ('Another', 'DET'),\n",
       " ('young', 'ADJ'),\n",
       " ('man', 'NOUN'),\n",
       " (',', '.'),\n",
       " ('my', 'DET'),\n",
       " ('dear', 'NOUN'),\n",
       " ('?', '.'),\n",
       " ('?', '.')]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sents[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#predict POS tags for all sentences in the testset\n",
    "y_pred = []\n",
    "\n",
    "for test_sent in input_test_sent:\n",
    "    temp_pred = predict(test_sent)\n",
    "    y_pred.append(temp_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pos_classification_report(y_true, y_pred):\n",
    " \n",
    "    lb = LabelBinarizer()\n",
    "    y_true_combined = lb.fit_transform(list(chain.from_iterable(y_true)))\n",
    "    y_pred_combined = lb.transform(list(chain.from_iterable(y_pred)))\n",
    "    tagset = sorted(set(lb.classes_)) \n",
    "    class_indices = {cls: idx for idx, cls in enumerate(lb.classes_)}\n",
    "    \n",
    "    return classification_report(\n",
    "        y_true_combined,\n",
    "        y_pred_combined,\n",
    "        labels = [class_indices[cls] for cls in tagset],\n",
    "        target_names = tagset,\n",
    "        digits=4\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          .     0.9998    1.0000    0.9999     15505\n",
      "        ADJ     0.7384    0.8645    0.7965      5492\n",
      "        ADP     0.9191    0.9276    0.9234      9630\n",
      "        ADV     0.8787    0.8830    0.8808      5357\n",
      "       CONJ     0.9937    0.9949    0.9943      3326\n",
      "        DET     0.9687    0.9797    0.9742     10113\n",
      "       NOUN     0.9471    0.8904    0.9179     17692\n",
      "        NUM     0.8134    0.9487    0.8758       487\n",
      "       PRON     0.9749    0.9763    0.9756      7353\n",
      "        PRT     0.8576    0.8160    0.8363      3446\n",
      "       VERB     0.9604    0.9614    0.9609     16917\n",
      "          X     0.1970    0.1048    0.1368       124\n",
      "\n",
      "avg / total     0.9405    0.9390    0.9392     95442\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(pos_classification_report(test_targets,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
